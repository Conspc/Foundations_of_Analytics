%!TeX encoding = UTF-8
%!TeX program = xelatex
\documentclass[notheorems, aspectratio=54]{beamer}
% aspectratio: 1610, 149, 54, 43(default), 32

\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{color,xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{amsthm}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\usepackage{lmodern} % 解决 font warning
% \usepackage[UTF8]{ctex}
\usepackage{animate} % insert gif

\usepackage{lipsum} % To generate test text 
\usepackage{ulem} % 下划线，波浪线

\usepackage{listings} % display code on slides; don't forget [fragile] option after \begin{frame}

% ----------------------------------------------
% tikx
\usepackage{framed}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{pgf}
\usetikzlibrary{automata, calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate, 
     decoration={random steps, segment length=2.5cm, amplitude=.7mm}},
  torn border/.style={orange!30!black!5, decorate, 
     decoration={random steps, segment length=.5cm, amplitude=1.7mm}}}

% Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border] 
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}    
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]                % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white, 
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

% ----------------------------------------------

\mode<presentation>{
    \usetheme{CambridgeUS}
    % Boadilla CambridgeUS
    % default Antibes Berlin Copenhagen
    % Madrid Montpelier Ilmenau Malmoe
    % Berkeley Singapore Warsaw
    \usecolortheme{beaver}
    % beetle, beaver, orchid, whale, dolphin
    \useoutertheme{infolines}
    % infolines miniframes shadow sidebar smoothbars smoothtree split tree
    \useinnertheme{circles}
    % circles, rectanges, rounded, inmargin
}
% 设置 block 颜色
\setbeamercolor{block title}{bg=red!30,fg=white}

\newcommand{\reditem}[1]{\setbeamercolor{item}{fg=red}\item #1}

% 缩放公式大小
\newcommand*{\Scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}

% 解决 font warning
\renewcommand\textbullet{\ensuremath{\bullet}}

% ---------------------------------------------------------------------
% flow chart
\tikzset{
    >=stealth',
    punktchain/.style={
        rectangle, 
        rounded corners, 
        % fill=black!10,
        draw=white, very thick,
        text width=6em,
        minimum height=2em, 
        text centered, 
        on chain
    },
    largepunktchain/.style={
        rectangle,
        rounded corners,
        draw=white, very thick,
        text width=10em,
        minimum height=2em,
        on chain
    },
    line/.style={draw, thick, <-},
    element/.style={
        tape,
        top color=white,
        bottom color=blue!50!black!60!,
        minimum width=6em,
        draw=blue!40!black!90, very thick,
        text width=6em, 
        minimum height=2em, 
        text centered, 
        on chain
    },
    every join/.style={->, thick,shorten >=1pt},
    decoration={brace},
    tuborg/.style={decorate},
    tubnode/.style={midway, right=2pt},
    font={\fontsize{10pt}{12}\selectfont},
}
% ---------------------------------------------------------------------

% code setting
\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{red},
    breaklines=true,
    xleftmargin=2em,
    numbers=left,
    numberstyle=\color[RGB]{222,155,81},
    frame=leftline,
    tabsize=4,
    breakatwhitespace=false,
    showspaces=false,               
    showstringspaces=false,
    showtabs=false,
    morekeywords={Str, Num, List},
}

% ---------------------------------------------------------------------

%% preamble
\title{Introduction to Machine Learning II}
% \subtitle{The subtitle}
\author{Dihui Lai}
\institute[WUSTL]{dlai@wustl.edu}

% -------------------------------------------------------------

\begin{document}

%% title frame
\begin{frame}
    \titlepage
\end{frame}

%% normal frame
\section{CART}
\begin{frame}
    \frametitle{CART: Impurity Measurement}
    At node $t$, the fraction of class $i$ is denoted as $p(i|t)$
    
    Entropy: $H(t)=-\sum\limits_{i=1}^{C}p(i|t)log(p(i|t))$
    
    Gini-Index: $Gini(t)=\sum\limits_{i=1}^{C}[p(i|t)]^2$
    
    Classification Error: $Error(t)=1-\max\limits_{i}p(i| t)$
\end{frame}

\begin{frame}
    \frametitle{Entropy}
    Considering the entropy of a data set that contains two types of outcomes: {0, 1}. Running 10 experiments, we get the following out comes (0, 0, 1, 0, 1, 0, 1, 1, 0, 1). The entropy is 
    $$
    H(t)=-\sum\limits_{i=1}^{2}\frac{1}{2}log_2(\frac{1}{2})=1
    $$
    
What if we have out come of (1, 1, 1, 1, 1, 1, 1, 1, 1, 1)? The entropy is 
    $$
    H(t)=1log_2(1)+0log_2(0)=0
    $$

The less variance in the data, the smaller the entropy is. For binary outcome, a probability of 1/2 leads to the largest uncertainty/entropy.

\end{frame}

\begin{frame}
\frametitle{CART Model}
John likes playing tennis on Saturday, The liklihood of John playing tennis depends on the weather. John has never played on a rainy. He may or may not play on a windy day. Given the weather, can you predict if John will play tennis this weekend?

\begin{center}
\begin{tikzpicture}
\tikzset{every tree node/.style={align=center,anchor=north}}
\Tree [.Rain? No 
		[.Hot? [.Humid? No Yes ][.Wind? No Yes ] ] ]
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{How to Grow a Tree?}
\begin{itemize}
\item At each node, the tree algorithm will check every variable for a possible split. 
\item A variable is selected for the split if it maximally reduces the impurity in the child nodes (e.g. the largest reduction of entroy, the largest reduction of variance etc.)
\item Pruning: a tree can grow till each leave only contains one data point. Pruning the tree is needed to avoid overfitting (e.g. level of depth, minimum sample number req) 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Information Gain at a Split Node}
Assuming we collect John's tennis activities for 20 weeks. Out of the 20 weekends, we have 8 sunny day and John plays tennis on all of them. Out of the rest 12 days, John played tennis on 7 days. 
Before any split, we have the entropy calculated as 
    $$
    H(root)=-\frac{15}{20}log(\frac{15}{20})-\frac{5}{20}log(\frac{5}{20})=0.562
    $$
Split based on the weather, we have on sunny days
$
H(sunny)=0
$
and on other days
$
H(!sunny)=-\frac{7}{12}log(\frac{7}{12})-\frac{5}{12}log(\frac{5}{12})=0.679
$
Therefore the child nodes have average entropy of 
$$
H(childs)=\frac{8}{20}\cdot 0+\frac{12}{20}\cdot 0.679=0.407
$$
We have entropy reduced by $\Delta H=0.562-0.390=0.154$
\end{frame}

\begin{frame}
\frametitle{Information Gain at a Split Node}
Instead of splitting by forcast, we look at the wind speed. Out of the 20 weekends, 10 days are windy and 10 days are not windy. Out of the windy day, John played 6 times and 9 times for the non-windy days.

Split based on the wind condition, we have on windy days
$
H(!windy)=-\frac{6}{10}log(\frac{6}{10})-\frac{4}{10}log(\frac{4}{10})=0.673
$
and on other days
$
H(windy)=\frac{9}{10}\cdot log(\frac{9}{10})+\frac{1}{10}\cdot log(\frac{1}{10})=0.325
$

Therefore the child nodes have average entropy of 
$$
H(childs)=\frac{1}{2}\cdot 0.673+\frac{1}{2}\cdot 0.325=0.499
$$
We have entropy reduced by $\Delta H=0.562-0.5=0.063$
\end{frame}


\begin{frame}
\begin{center}
At the first split, what condition shall we use to do the split?
\end{center}

\end{frame}


\section{Random Forest}

\section{GBM}


%% normal frame
\section{Neural Network}
\begin{frame}
    \center{Introduction to Neural Network}
\end{frame}
\begin{frame}
\frametitle{Neural Network: Topology}

\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=0.8cm
  },
  neuron missing/.style={
    draw=none, 
    scale=2,
    text height=0.2cm,
    execute at begin node=\color{black}$\vdots$
  },
}

\begin{figure}
\centering
\begin{tikzpicture}[x=1.5cm, y=1.1cm, >=stealth]

\foreach \m/\l [count=\y] in {1,2,3,missing,4}
  \node [every neuron/.try, neuron \m/.try] (input-\m) at (0, 2-\y*0.8) {};

\foreach \m [count=\y] in {1, 2, 3, 4, missing, 5}
  \node [every neuron/.try, neuron \m/.try ] (hiddenl-\m) at (1.5, 2.5-\y*0.8) {};
  
\foreach \m [count=\y] in {1, 2, missing, 3}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (3, 2.3-\y) {};
  
\foreach \m [count=\y] in {1,2, missing, 3}
  \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4.5,2.3-\y) {};

\foreach \l [count=\i] in {1,2,3, n}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$I_\l$};

\foreach \l [count=\i] in {1, 2, 3, 4, n}
  \node [above] at (hiddenl-\i.south) {$z^{l}_\l$};

\foreach \l [count=\i] in {1, 2, n}
  \node [above] at (hidden-\i.south) {$z^{l+1}_\l$};
  
\foreach \l [count=\i] in {1,2, n}
  \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$O_\l$};

\foreach \i in {1,...,4}
  \foreach \j in {1,...,5}
    \draw [->] (input-\i) -- (hiddenl-\j);
    
\foreach \i in {1,...,5}
  \foreach \j in {1,...,3}
    \draw [->] (hiddenl-\i) -- (hidden-\j);
    
\foreach \i in {1,...,3}
  \foreach \j in {1,...,3}
    \draw [->] (hidden-\i) -- (output-\j);

\foreach \l [count=\x from 0] in {Input, l, l+1, output}
  \node [align=center, above] at (\x*1.5,2) {\l};

\end{tikzpicture}
\end{figure}

\end{frame}

\begin{frame}


\frametitle{Neural Network: Forward}
    Each neuron at layer $l$ recieves inputs from all neuron from the previous layer $l-1$
    $$
    z_k^l=\sum_jw^{l-1}_{kj}a_j^{l-1}
    $$
  	The neuron tansfer the input signal $z_k^l$ via a transfer function $\sigma$ and send as input to to the next layer
    $$
	a_k^l=\sigma(z_k^l)
    $$
    The cost function of the neural network is dependeng on all the $z$s of neurons in all layers
    $$
    C\left(z^l_1, z^l_2, ... z^l_k(z^{l-1}_1, z^{l-1}_2, z^{l-1}_3, ...), ...\right)
    $$

\end{frame}
\begin{frame}
\frametitle{Neural Network: Backpropogation}

\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1cm
  },
  neuron missing/.style={
    draw=none, 
    scale=1,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}

\begin{figure}
\centering
\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
\foreach \m [count=\y] in {1, 2, missing, 3}
  \node [every neuron/.try, neuron \m/.try ] (hiddenl-\m) at (2, 2.5-\y*0.8) {};
  
\foreach \m [count=\y] in {2}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (3.5, 1.8-\y) {$z^{l}_k|a_k^{l}$};
  
\foreach \m [count=\y] in {1,2, missing, 3}
  \node [every neuron/.try, neuron \m/.try ] (output-\m) at (5, 2.5-\y*0.8) {};

\foreach \l [count=\i] in {1, 2, n}
  \node [above] at (hiddenl-\i.south) {$a^{l-1}_{\l}$};
  
\foreach \l [count=\i] in {1, 2, n}
  \node [above] at (output-\i.south) {$z^{l+1}_{\l}$};
  
\foreach \i in {1,...,3}
  \foreach \j in {2}
    \draw [<-] (hiddenl-\i) -- (hidden-\j);

\foreach \i in {2}
  \foreach \j in {1,...,3}
    \draw [<-] (hidden-\i) -- (output-\j);
\end{tikzpicture}
\end{figure}


\end{frame}
\begin{frame}

\frametitle{Neural Network: Backpropogation}
The contribution to the cost function from a neuron in layer $l$ can be cauculated iteratively as
\begin{align*}
\delta^l_k=\frac{\partial C}{\partial z_k^l}&=\sum_m \frac{\partial C}{\partial z_m^{l+1}}\frac{\partial z_m^{l+1}}{\partial z_k^l}\\
&=\left(\sum_m \frac{\partial C}{\partial z_m^{l+1}}\frac{\partial z_m^{l+1}}{\partial a_k^l}\right)\frac{\partial a_k^l}{\partial z_k^l}\\
&=\sum_m \delta^{l+1}_m w^l_{mk}\sigma'(z_k^l)
\end{align*}
The partial derivative of a cost function w.r.t the weight $w_{kj}^{l-1}$ is 
$$
\frac{\partial C}{\partial w_{kj}^{l-1}}=\frac{\partial C}{\partial z_k^l}\frac{\partial z_k^l}{\partial w_{kj}^{l-1}}=\delta^l_k a^{l-1}_j
$$
\end{frame}

\end{document}
