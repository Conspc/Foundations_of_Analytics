%!TeX encoding = UTF-8
%!TeX program = xelatex
\documentclass[notheorems, aspectratio=54]{beamer}
% aspectratio: 1610, 149, 54, 43(default), 32

\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{color,xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{lmodern} % 解决 font warning
% \usepackage[UTF8]{ctex}
\usepackage{animate} % insert gif

\usepackage{lipsum} % To generate test text 
\usepackage{ulem} % 下划线，波浪线

\usepackage{listings} % display code on slides; don't forget [fragile] option after \begin{frame}

% ----------------------------------------------
% tikx
\usepackage{framed}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{automata, calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate, 
     decoration={random steps, segment length=2.5cm, amplitude=.7mm}},
  torn border/.style={orange!30!black!5, decorate, 
     decoration={random steps, segment length=.5cm, amplitude=1.7mm}}}

% Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border] 
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}    
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]                % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white, 
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

% ----------------------------------------------

\mode<presentation>{
    \usetheme{CambridgeUS}
    % Boadilla CambridgeUS
    % default Antibes Berlin Copenhagen
    % Madrid Montpelier Ilmenau Malmoe
    % Berkeley Singapore Warsaw
    \usecolortheme{beaver}
    % beetle, beaver, orchid, whale, dolphin
    \useoutertheme{infolines}
    % infolines miniframes shadow sidebar smoothbars smoothtree split tree
    \useinnertheme{circles}
    % circles, rectanges, rounded, inmargin
}
% 设置 block 颜色
\setbeamercolor{block title}{bg=red!30,fg=white}

\newcommand{\reditem}[1]{\setbeamercolor{item}{fg=red}\item #1}

% 缩放公式大小
\newcommand*{\Scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}

% 解决 font warning
\renewcommand\textbullet{\ensuremath{\bullet}}

% ---------------------------------------------------------------------
% flow chart
\tikzset{
    >=stealth',
    punktchain/.style={
        rectangle, 
        rounded corners, 
        % fill=black!10,
        draw=white, very thick,
        text width=6em,
        minimum height=2em, 
        text centered, 
        on chain
    },
    largepunktchain/.style={
        rectangle,
        rounded corners,
        draw=white, very thick,
        text width=10em,
        minimum height=2em,
        on chain
    },
    line/.style={draw, thick, <-},
    element/.style={
        tape,
        top color=white,
        bottom color=blue!50!black!60!,
        minimum width=6em,
        draw=blue!40!black!90, very thick,
        text width=6em, 
        minimum height=2em, 
        text centered, 
        on chain
    },
    every join/.style={->, thick,shorten >=1pt},
    decoration={brace},
    tuborg/.style={decorate},
    tubnode/.style={midway, right=2pt},
    font={\fontsize{10pt}{12}\selectfont},
}
% ---------------------------------------------------------------------

% code setting
\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{red},
    breaklines=true,
    xleftmargin=2em,
    numbers=left,
    numberstyle=\color[RGB]{222,155,81},
    frame=leftline,
    tabsize=4,
    breakatwhitespace=false,
    showspaces=false,               
    showstringspaces=false,
    showtabs=false,
    morekeywords={Str, Num, List},
}

% ---------------------------------------------------------------------

%% preamble
\title{Linear Model (LM) and Generalized Linear Model (GLM)}
% \subtitle{The subtitle}
\author{Dihui Lai}
\institute[WUSTL]{dlai@wustl.edu}

% -------------------------------------------------------------

\begin{document}

%% title frame
\begin{frame}
    \titlepage
\end{frame}


\begin{frame}
\frametitle{Content}

\begin{itemize}
\item Random Variables: Dependent, Independent, Correlation 
\item Linear Regression of One Variable
\item Linear Regression of Multiple Variables
\item Logistic Regression
\end{itemize}
\end{frame}


%% normal frame
\section{Random Variables: Dependent, Independent, Correlation }

\begin{frame}

\frametitle{Relationships between Random Variables}

Let's look at a few pairs of data  points?

\begin{itemize}
\item $\textbf{X}=[0.5, 0.6, 0.1, -0.3, 2.3], \textbf{Y}=[0.5, 0.6, 0.1, -0.3, 2.3]$
\item $\textbf{X}=[0.5, 0.6, 0.1, -0.3, 2.3], \textbf{Y}=[0.6, 0.6, 0.12, -0.3, 2.3]$
\item $\textbf{X}=[0.5, 0.6, 0.1, -0.3, 2.3], \textbf{Y}=[0.02, -0.2,0.2,2.1, -0.5]$
\end{itemize}

What can you tell about the relationship between \textbf{X} and \textbf{Y}?

\end{frame}

\begin{frame}
\frametitle{Relationships between Random Variables}
Given two random variables $X$ and $Y$, denote the mean and variance of the two variables as $E[X]=\mu_X$, $E[Y]=\mu_Y$, $Var[X]=\sigma^2_X$, $Var[Y]=\sigma^2_Y$.

The covariance of X and Y is the number defined by
\begin{align*}
Cov(X, Y)&=E[(X-\mu_X)(Y-\mu_Y)]\\
&=E[XY]-\mu_X \mu_Y
\end{align*}

The correlation of the two random variables is the number defined by 


\begin{align*}
\rho_{XY}=\frac{Cov(X,Y)}{\sigma_X \sigma_Y}
\end{align*}

\end{frame}



\begin{frame}
\frametitle{Relationships between Random Variables}
Calculate the covariance/correlation of 
\begin{itemize}
\item Example 1:$$\textbf{X}=[2, -2, -2, 2], \textbf{Y}=[2, -2, -2, 2]$$
We have
$\mu_X=0$, $\mu_Y=0$, $\sigma_X^2=4$, $\sigma_Y^2=4$, $E[XY]=4$
Therefore $Cov(X, Y)=4-0=4$ and $\rho_{XY}=4/(2*2)=1$

\item Example 2: $$\textbf{X}=[2, -2, -2, 2], \textbf{Y}=[2, 0, -2, 0]$$
We have
$\mu_X=0$, $\mu_Y=0$, $\sigma_X^2=4$, $\sigma_Y^2=2$, $E[XY]=2$
Therefore $Cov(X, Y)=2-0=2$ and $\rho_{XY}=2/(2*\sqrt{2})=1/\sqrt{2}$
\end{itemize}



\end{frame}

\section{Univariate LM}

\begin{frame}

\frametitle{Linear Regression with One Variable}
Data set:
$$
\textbf{Y}=
\begin{bmatrix}
    y^{(1)}\\
    y^{(2)}\\
	\vdots\\
    y^{(n)}
\end{bmatrix}
,
\textbf{X}=
\begin{bmatrix}
    x^{(1)}\\
    x^{(2)}\\
	\vdots\\
    x^{(n)}
\end{bmatrix}
$$
\end{frame}

\begin{frame}
\frametitle{Linear Regression with One Variable}
Assume y is linearly depending on x i.e.

$$
y = \beta_0 + \beta_1 x
$$

Find $\hat{\beta}$ that minimize the estimation error

$$
J=\sum\limits_{j=1}^{n}(y^{(j)}-\hat{y}^{(j)})^2 = \sum\limits_{j=1}^{n} (y^{(j)}-\hat{\beta_0}-\hat{\beta_1} x^{(j)})^2
$$

i.e.

$$
\frac{\partial J}{\partial \hat{\beta}_1}=0
\rightarrow
\sum\limits_{j=1}^{n}(y^{(j)}-\hat{\beta_0}-\hat{\beta_1} x^{(j)})x^{(j)}=0
$$
$$
\frac{\partial J}{\partial \hat{\beta}_0}=0
\rightarrow
\sum\limits_{j=1}^{n}(y^{(j)}-\hat{\beta_0}-\hat{\beta_1} x^{(j)})=0
$$

\end{frame}

\begin{frame}

$$
\hat{\beta_0}\sum\limits_{j=1}^{n}x^{(j)}=\sum\limits_{j=1}^{n}y^{(j)}x^{(j)}-\hat{\beta_1}\sum\limits_{j=1}^{n}x^{(j)}x^{(j)}
$$
\begin{align*}
\hat{\beta_0}=\frac{1}{n}\sum\limits_{j=1}^{n}(y^{(j)}-\hat{\beta_1} x^{(j)})=\bar{y}-\hat{\beta_1} \bar{x}
\end{align*}

Insert the second equation to the first, we have

$$n\bar{x}\bar{y}-\bar{\beta_1}n\bar{x}\bar{x}=\sum\limits_{j=1}^{n}y^{(j)}x^{(j)}-\hat{\beta_1}\sum\limits_{j=1}^{n}x^{(j)}x^{(j)}
$$

Therefore, 
$$\hat{\beta_1}=\frac{\frac{1}{n}\sum\limits_{j=1}^{n}y^{(j)}x^{(j)}-\bar{x}\bar{y}}{\frac{1}{n}\sum\limits_{j=1}^{n}x^{(j)}x^{(j)}-\bar{x}^2}=\frac{Cov(X,Y)}{Var(X)}=\rho_{XY}\frac{\sigma_{Y}}{\sigma_{X}}$$
\end{frame}

\section{Multivariate LM}
\begin{frame}
\frametitle{Multivariate Linear Regression}
Data set:

$$
\begin{bmatrix}

\textbf{Y}, \textbf{X}

\end{bmatrix}
=
\begin{bmatrix}
    y^{(1)}& x^{(1)}_{0}& x^{(1)}_{1} & x^{(1)}_{2} & \dots & x^{(1)}_{p} \\
    y^{(2)}& x^{(2)}_{0}& x^{(2)}_{1} & x^{(2)}_{2} & \dots & x^{(2)}_{p} \\
	\vdots & \vdots     & \vdots & \vdots &\ddots &\vdots\\
    y^{(n)}&  x^{(n)}_{0}& x^{(n)}_{1} & x^{(n)}_{2} & \dots & x^{(n)}_{p}
\end{bmatrix}
$$
or explicitly 

$$
\textbf{Y}=
\begin{bmatrix}
    y^{(1)} \\
    y^{(2)} \\
	\vdots \\
    y^{(n)}
\end{bmatrix}
,
\textbf{X}=
\begin{bmatrix}

    x^{(1)}_{0}& x^{(1)}_{1} & x^{(1)}_{2} & \dots & x^{(1)}_{p} \\
    x^{(2)}_{0}& x^{(2)}_{1} & x^{(2)}_{2} & \dots & x^{(2)}_{p} \\
	\vdots & \vdots & \vdots &\ddots &\vdots\\
  	x^{(n)}_{0}& x^{(n)}_{1} & x^{(n)}_{2} & \dots & x^{(n)}_{p}
\end{bmatrix}
$$


\end{frame}

\begin{frame}
\frametitle{Multivariate Linear Regression}
Assume y is a linear superposition of multiple x's
    $$
	y=\beta_{0}+\beta_{1}x_1+\beta_{2}x_2+\hdots+\beta_{p}x_p
    $$
    or simply
    $$
	y=\sum\limits_{i=1}^{p}\beta_i x_i     
	$$
        
   	Estimate $\hat{\beta}$'s that best fits the data i.e.
   	For each estimated data point
   	$$
	\hat{y}^{(j)}=\sum\limits_{i=1}^{p}\hat{\beta}_i x^{(j)}_i     
	$$
    we need to minimize the error 
    $$
    J(\beta)=\sum\limits_{j=1}^{n}(y^{(j)}-\hat{y}^{(j)})^2
    $$

\end{frame}

\begin{frame}
\frametitle{Multivariate Linear Regression}

Solve the optimization problem in matrix format
$$
\frac{\partial J}{\partial \beta_i}=0
$$
i.e.
$$
\sum\limits_{j=1}^{n}\frac{\partial (y^{(j)}-\hat{y}^{(j)})^2}{\partial \beta_i}=0
$$

$$
\sum\limits_{j=1}^{n}(y^{(j)}-\hat{y}^{(j)})\frac{\partial\hat{y}^{(j)}}{\partial \beta_i}=0
$$

$$
\sum\limits_{j=1}^{n}(y^{(j)}-\hat{y}^{(j)})x^{(j)}_{i}=0
$$

\end{frame}

\begin{frame}

\frametitle{Multivariate Linear Regression}

written in matrix formula we have

$$
J(\beta)=\mathbf{({Y}-{X\hat{\beta}})}^{T}\mathbf{X}
$$

$$
\mathbf{X}^{T}\mathbf{Y}-\mathbf{X}^T\mathbf{X}\hat{\beta}=0
$$

Therefore 

$$
\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{Y}
$$

\end{frame}

\section{Logistic Regression}

\begin{frame}

\frametitle{Logistic Regression}
\begin{align*}
p&=\frac{1}{1+\exp(-\vec{\beta}\cdot\vec{x}^j)}\\
L&=\prod_{j=1}^{n}p_{j}^{y_j}(1-p_{j})^{1-y_{j}}\\
\ell&=\log(L)\\
&=\sum_{j=1}^{n}y_j\log(p_j)+(1-y_j)\log(1-p_j)\\
&=\sum_{j=1}^{n}y_j\log\frac{p_j}{1-p_j}+\log(1-p_j)\\
&=\sum_{j=1}^{n}y^j(\vec{\beta}\cdot\vec{x}^j)-\log(1+\exp(\vec{\beta}\cdot\vec{x}^j)))
\end{align*}

\end{frame}


\begin{frame}

\frametitle{Logistic Regression}
\begin{align*}
\frac{\partial\ell}{\partial\beta_i}
&=\sum_{j=1}^{n}y^jx_{i}^{j}-\frac{x_{i}^j}{1+\exp(-\vec{\beta}\cdot\vec{x}^j)}\\
&=\sum_{j=1}^{n}\left (y^j-\frac{1}{1+\exp(-\vec{\beta}\cdot\vec{x}^j)} \right ) x_{i}^{j}\\
&=\sum_{j=1}^{n}\left (y^j-\hat{y}^j \right ) x_{i}^{j}\\
\nabla\ell&=\textbf{X}^T(\textbf{Y}-\hat{\textbf{Y}})
\end{align*}
$\beta$s can not be solved by setting $\nabla\ell=0$ because of the nonlinear formula for $\hat{y}^j=\frac{1}{1+\exp(\vec{x}^j\cdot\vec{\beta}^j)}$. Recall $\hat{y}^j=\vec{x}^j\cdot\vec{\beta}^j$ for linear regression.
\end{frame}

\begin{frame}

\subsection{Newton-Raphson Method}

\frametitle{Newton-Raphson Method for Optimizatoin}
Consider a function of one parameter $f(\beta)$ and assume $\beta_0$ is close to the point that minimizes $f(\beta)$. We can therefore use Talyor expansion for approximation

$$
f(\beta)=f(\beta_0)+f'(\beta_0)(\beta-\beta_0)+\frac{1}{2}f''(\beta_0)(\beta-\beta_0)^2
$$

The $\beta^*$ that minimize the function have derivative at the point 0 i.e. $f'(\beta)|_{\beta=\beta^*}=0$, by setting $f'(\beta)=0$, we get an iterative evaluation methods for $\beta^*$

$$
f'(\beta_0)+\frac{1}{2}2f''(\beta_0)(\beta-\beta_0)=0 
$$
$$
\rightarrow
\beta=\beta_0-\frac{f'(\beta_0)}{f''(\beta_0)} \text{i.e.}
$$

$$
\beta^{(m+1)}=\beta^{(m)}-\frac{f'(\beta^{(m)})}{f''(\beta^{(m)})}
$$
\end{frame}

\begin{frame}
\frametitle{Multivariate Newton-Raphson Method }
For multivarite function, the iteration formula becomes
$$
\beta^{(m+1)}=\beta^{(m)}-H^{-1}(\beta^{(m)})\nabla f(\beta^{(m)})\text{,}
$$
here $H(\beta^{(m)})$ is the Hessian matrix of $f(\beta)$ evaluated at $\beta=\beta^{(m)}$, defined as

$$
H_{ij}=\frac{\partial^2f}{\partial\beta_i\partial\beta_j}|_{\beta=\beta^{(m)}}
$$
and $H^{-1}(\beta^{(m)})$ is the inverse of $H(\beta^{(m)})$

\end{frame}

\begin{frame}

\frametitle{Logistic Regression}
Apply Newton-Raphson methods to optimize the logistic regression, we calculate the Hessian of the log-likelihood function
\begin{align*}
\frac{\partial^2\ell}{\partial\beta_k\partial\beta_i}
&=-\sum_{j=1}^{n}x_i^j\frac{\exp(-\vec{\beta}\cdot\vec{x}^j)}{(1+\exp(-\vec{\beta}\cdot\vec{x}^j))^2}x_k^j\\
&=-\sum_{j=1}^{n}x_i^j \hat{y}^j (1-\hat{y}^j)x_k^j\\
\end{align*}
written in matrix formula
\begin{align*}
\textbf{H}&=-\textbf{X}^T\textbf{W}\textbf{X}\text{, } 
\textbf{W}=  \begin{bmatrix}
    \hat{y}^1 (1-\hat{y}^1) & & \\
    & \ddots & \\
    & & \hat{y}^n (1-\hat{y}^n)
  \end{bmatrix}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Logistic Regression}
\begin{align*}
\vec{\beta}^{(m+1)} &\leftarrow \vec{\beta}^{(m)}-\textbf{H}^{-1}\nabla\ell\\
\vec{\beta}^{(m+1)} &\leftarrow \vec{\beta}^{(m)}+(\textbf{X}^T\textbf{W}\textbf{X})^{-1}\textbf{X}^T(\textbf{Y}-\hat{\textbf{Y}})
\end{align*}

by defining  $z^{(m)}=\textbf{X}{\beta}^{(m)}+\textbf{W}^{-1}(\textbf{Y}-\hat{\textbf{Y}})$ we have
\begin{align*}
\vec{\beta}^{(m+1)} &\leftarrow (\textbf{X}^T\textbf{W}\textbf{X})^{-1}\textbf{X}^T\textbf{W}z^{(m)}
\end{align*}

Recall in linear regression case
$$\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{Y}$$
\end{frame}

\section{Generalized Linear Model}

\begin{frame}
\frametitle{Generalized Linear Model}
Exponential family of probability density function
$$
f(y)=\exp\left( \frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)
$$
The distribution have the following properties
\begin{itemize}
\item $E(Y)=b'(\theta)$
\item $Var(Y)=b''(\theta)a(\phi)$
\end{itemize}
\end{frame}

\begin{frame}

\frametitle{Generalized Linear Model: Gaussian}
Gaussian distribution as a special case of exponential family 
$$
f(y)=\exp\left( \frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-\frac{y^2}{2\sigma^2}-\frac{1}{2}\log(2\pi\sigma^2)\right)
$$

where we have $a(\phi)=\sigma^2$, $b(\mu)=\frac{1}{2}\mu^2$
Therefore
\begin{itemize}
\item $E(Y)=b'(\theta)=\mu$
\item $Var(Y)=b''(\theta)a(\phi)=\sigma^2$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Link Function}
Assume a linear model where 
\begin{align*}
\theta&=\eta=\vec{x}\cdot\vec{\beta}\\
b'(\theta)&=\mu=g(\vec{x}\cdot\vec{\beta})
\end{align*}
here $g^{-1}(\cdot)$ is the link function

\end{frame}

\begin{frame}
\frametitle{Log Likelihood Function of GLM}
The likelihood function of GLM

$$\ell=\sum_{j}\frac{y^j\theta^j-b(\theta^j)}{a(\phi)}+c^j(y^j, \phi)$$

In the model, only $\theta$ is depedent on $\vec{x}\cdot\vec{\beta}$. Therefore, "maximize" the likelihood function is equivalent to maximize 
$$\ell=\sum_{j} \left[y^j\theta^j-b(\theta^j)\right]$$

\end{frame}

\begin{frame}
\frametitle{Log Likelihood Function of GLM}

Let's consider each data point and its contribution to the likelihood function
$$\ell^j=y^j\theta^j-b(\theta^j)$$ or simplified as $$\ell=y\theta-b(\theta)$$
Using Newton-Raphson method
$$
\beta^{(m+1)}=\beta^{(m)}-H^{-1}(\beta^{(m)})\nabla \ell(\beta^{(m)})\text{,}
$$
We need to calculate the gradient of $\ell$ and its Hessian
\end{frame}

\begin{frame}
\frametitle{The Gradient and Hessian}
The gradient can be derived as
$$\frac{\partial \ell}{\partial \beta_i}=-2\sum_j(y^j-\mu^j)\frac{g'(\eta^j)}{V(\mu)^j}x_i^j$$

The hessian can be derived as
$$\frac{\partial^2 \ell}{\partial \beta_k\partial \beta_i}=2\sum_j\left[\frac{g'(\eta^j)^2}{V(\mu^j)}-(y^j-\mu^j)\frac{g''(\eta^j)V(\mu^j)-g'(\eta^j)^2V'(\mu^j)}{V(\mu^j)^2}\right]x_i^jx_k^j$$
\end{frame}


\begin{frame}

\frametitle{Optimization: Gradient Descent Method}
Cost function $J(\beta)$

Update methods
$$\beta_i\leftarrow \beta_i-\epsilon\frac{\partial}{\partial\beta_i}J(\beta)$$
where $\epsilon$ is the learning rate
\end{frame}	


\begin{frame}

\frametitle{Gradient Descent Method for Linear Regression}
Cost function $J(\beta)=\sum_j\frac{1}{2}(y^j-\vec{x}^j\cdot\vec{\beta})^2$

$$\frac{\partial J}{\partial \beta_i}=\sum_j(\hat{y}^j-y^j)x_i^j$$

Update methods is now
$$\beta_i\leftarrow \beta_i+\epsilon(y^j-\hat{y}^j)x_i^j$$

The update method is quite intuitive considering that $\beta_i$ is adjusted higher if estimated $\hat{y}^j$ is less than $y^j$; adjusted lower if $\hat{y}^j$ is more than $y^j$

\end{frame}	


\begin{frame}

\frametitle{Batch/Stochastic Gradient Descent}
\textbf{Batch Gradient Descent}: if each step $\beta_i$ is updated using all data points

$$\beta_i\leftarrow \beta_i+\sum_j\epsilon\frac{\partial}{\partial\beta_i}J(\beta)$$ or
$$\beta_i\leftarrow \beta_i+\sum_j\epsilon(y^j-\hat{y}^j)x_i^j$$


\textbf{Stochastic Gradient Descent}: if each step $\beta_i$ is updated using only one data point

$$\beta_i\leftarrow \beta_i+\epsilon\frac{\partial}{\partial\beta_i}J(\beta)$$ or
$$\beta_i\leftarrow \beta_i+\epsilon(y^j-\hat{y}^j)x_i^j$$


\end{frame}	


\end{document}
