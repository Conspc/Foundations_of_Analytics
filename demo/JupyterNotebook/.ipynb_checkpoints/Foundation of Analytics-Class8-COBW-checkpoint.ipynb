{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New package installation\n",
    "```pip install tensorflow_datasets```\n",
    "\n",
    "```pip install ipywidgets```\n",
    "\n",
    "```pip install nltk```\n",
    "### Reference:\n",
    "https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Data/J. K. Rowling - Harry Potter 1 - Sorcerer's Stone\",'r')\n",
    "raw_data_1 = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import nltk \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = wordpunct_tokenize(raw_data_1)  \n",
    "word_tokens = [w.lower() for w in word_tokens if not w in stop_words] \n",
    "word_tokens = [w.lower() for w in word_tokens if w.isalpha()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word_tokens)\n",
    "char_to_int = dict((c,i) for i,c in enumerate(vocab))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([], dtype=np.float).reshape(0,4)\n",
    "Y = np.array([])\n",
    "Xwords=[]\n",
    "Ywords=[]\n",
    "window_size = 2\n",
    "for i, word in enumerate(word_tokens):\n",
    "    Xsub=np.zeros(2*window_size)\n",
    "    Xsubwords=[]\n",
    "    isetvalue=0\n",
    "    for icontext in range(max(i-window_size,0), min(i+window_size, len(word_tokens)-1)+1):\n",
    "        if icontext!=i:\n",
    "            Xsub[isetvalue]=char_to_int[word_tokens[icontext]]\n",
    "            Xsubwords.append(word_tokens[icontext])\n",
    "            isetvalue=isetvalue+1\n",
    "    X=np.vstack([X, Xsub])\n",
    "    Xwords.append(Xsubwords)\n",
    "    Y=np.append(Y,char_to_int[word])\n",
    "    Ywords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "vocab_size=len(vocab)\n",
    "cbowNN = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbowNN.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45231 samples\n",
      "Epoch 1/10\n",
      "45231/45231 [==============================] - 23s 504us/sample - loss: 7.6901 - accuracy: 0.0287\n",
      "Epoch 2/10\n",
      "45231/45231 [==============================] - 21s 465us/sample - loss: 7.2384 - accuracy: 0.0329\n",
      "Epoch 3/10\n",
      "45231/45231 [==============================] - 22s 487us/sample - loss: 7.0855 - accuracy: 0.0475\n",
      "Epoch 4/10\n",
      "45231/45231 [==============================] - 22s 486us/sample - loss: 6.8970 - accuracy: 0.0585\n",
      "Epoch 5/10\n",
      "45231/45231 [==============================] - 22s 496us/sample - loss: 6.6652 - accuracy: 0.0669\n",
      "Epoch 6/10\n",
      "45231/45231 [==============================] - 22s 491us/sample - loss: 6.3901 - accuracy: 0.0798\n",
      "Epoch 7/10\n",
      "45231/45231 [==============================] - 23s 514us/sample - loss: 6.0882 - accuracy: 0.0946\n",
      "Epoch 8/10\n",
      "45231/45231 [==============================] - 24s 531us/sample - loss: 5.7696 - accuracy: 0.1113\n",
      "Epoch 9/10\n",
      "45231/45231 [==============================] - 23s 519us/sample - loss: 5.4438 - accuracy: 0.1295\n",
      "Epoch 10/10\n",
      "45231/45231 [==============================] - 234s 5ms/sample - loss: 5.1179 - accuracy: 0.1504\n"
     ]
    }
   ],
   "source": [
    "history = cbowNN.fit(X, Y,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5730, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbowNN.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5059"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int[\"harry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int[\"potter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5238643 ,  0.30421928,  0.33147296, -0.634414  , -0.5769926 ,\n",
       "        0.06840047,  0.22078611, -0.08689712, -0.6194999 ,  0.28277722,\n",
       "        0.44462878,  0.4402804 ,  0.37890956,  0.44396782, -0.55256784,\n",
       "       -0.5538398 , -0.31861556,  0.55215275,  0.15342586,  0.6087728 ,\n",
       "        0.6459889 , -0.47282505, -0.26229027, -0.6077571 ,  0.6792281 ,\n",
       "       -0.2994563 ,  0.1866997 , -0.46849182, -0.33732918, -0.47328424,\n",
       "        0.5500591 ,  0.33660218,  0.3826662 ,  0.42157295,  0.5949989 ,\n",
       "        0.29590735,  0.4989111 ,  0.4032873 , -0.52393204, -0.25388965,\n",
       "        0.3997271 , -0.62224257, -0.2616375 , -0.3902542 ,  0.42076963,\n",
       "       -0.56467456,  0.04902203,  0.49545494, -0.29873022,  0.2472937 ,\n",
       "        0.42883047, -0.50888944, -0.03138658,  0.10933125,  0.4736284 ,\n",
       "        0.20750256, -0.42921713, -0.3238205 , -0.5687047 , -0.64927644,\n",
       "       -0.4396635 , -0.5443118 ,  0.39546123, -0.46802753,  0.58282965,\n",
       "       -0.30756986, -0.27993557,  0.21802768,  0.4000741 , -0.29830384,\n",
       "        0.508375  ,  0.40352774, -0.36644813, -0.40735883, -0.5541513 ,\n",
       "       -0.6472093 ,  0.12224057, -0.355367  ,  0.6025891 ,  0.16887368,\n",
       "        0.17137162,  0.30529475, -0.55577934,  0.23873103, -0.4515271 ,\n",
       "       -0.5173838 , -0.47880873,  0.50740194,  0.03964925, -0.46314573,\n",
       "        0.06023937, -0.5860773 , -0.43632504,  0.30489495,  0.41983265,\n",
       "       -0.5577319 , -0.66357225,  0.2190148 , -0.6292028 ,  0.28775424],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "e = cbowNN.layers[0]\n",
    "e(tf.constant(1168)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wharry=e.get_weights()[0][5059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpotter=e.get_weights()[0][691]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9999998 , -0.04303762],\n",
       "       [-0.04303762,  1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "cosine_similarity([wharry, wpotter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potter\n",
      "felt\n",
      "spun\n",
      "forehead\n",
      "bedspread\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(cbowNN.predict(tf.constant([5059]))[0])[::-1][:5]:\n",
    "    print(int_to_char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
