%!TeX encoding = UTF-8
%!TeX program = xelatex
\documentclass[notheorems, aspectratio=54]{beamer}
% aspectratio: 1610, 149, 54, 43(default), 32

\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{color,xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{lmodern} % 解决 font warning
% \usepackage[UTF8]{ctex}
\usepackage{animate} % insert gif

\usepackage{lipsum} % To generate test text 
\usepackage{ulem} % 下划线，波浪线

\usepackage{listings} % display code on slides; don't forget [fragile] option after \begin{frame}

% ----------------------------------------------
% tikx
\usepackage{framed}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{automata, calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate, 
     decoration={random steps, segment length=2.5cm, amplitude=.7mm}},
  torn border/.style={orange!30!black!5, decorate, 
     decoration={random steps, segment length=.5cm, amplitude=1.7mm}}}

% Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border] 
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}    
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]                % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white, 
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

% ----------------------------------------------

\mode<presentation>{
    \usetheme{CambridgeUS}
    % Boadilla CambridgeUS
    % default Antibes Berlin Copenhagen
    % Madrid Montpelier Ilmenau Malmoe
    % Berkeley Singapore Warsaw
    \usecolortheme{beaver}
    % beetle, beaver, orchid, whale, dolphin
    \useoutertheme{infolines}
    % infolines miniframes shadow sidebar smoothbars smoothtree split tree
    \useinnertheme{circles}
    % circles, rectanges, rounded, inmargin
}
% 设置 block 颜色
\setbeamercolor{block title}{bg=red!30,fg=white}

\newcommand{\reditem}[1]{\setbeamercolor{item}{fg=red}\item #1}

% 缩放公式大小
\newcommand*{\Scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}

% 解决 font warning
\renewcommand\textbullet{\ensuremath{\bullet}}

% ---------------------------------------------------------------------
% flow chart
\tikzset{
    >=stealth',
    punktchain/.style={
        rectangle, 
        rounded corners, 
        % fill=black!10,
        draw=white, very thick,
        text width=6em,
        minimum height=2em, 
        text centered, 
        on chain
    },
    largepunktchain/.style={
        rectangle,
        rounded corners,
        draw=white, very thick,
        text width=10em,
        minimum height=2em,
        on chain
    },
    line/.style={draw, thick, <-},
    element/.style={
        tape,
        top color=white,
        bottom color=blue!50!black!60!,
        minimum width=6em,
        draw=blue!40!black!90, very thick,
        text width=6em, 
        minimum height=2em, 
        text centered, 
        on chain
    },
    every join/.style={->, thick,shorten >=1pt},
    decoration={brace},
    tuborg/.style={decorate},
    tubnode/.style={midway, right=2pt},
    font={\fontsize{10pt}{12}\selectfont},
}
% ---------------------------------------------------------------------

% code setting
\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{red},
    breaklines=true,
    xleftmargin=2em,
    numbers=left,
    numberstyle=\color[RGB]{222,155,81},
    frame=leftline,
    tabsize=4,
    breakatwhitespace=false,
    showspaces=false,               
    showstringspaces=false,
    showtabs=false,
    morekeywords={Str, Num, List},
}

% ---------------------------------------------------------------------

%% preamble
\title{Foundation of Analytics: Lecture 4}
% \subtitle{The subtitle}
\author{Dihui Lai}
\institute[WUSTL]{dlai@wustl.edu}

\begin{document}

%% title frame
\begin{frame}
    \titlepage
\end{frame}

\section{Logistic Regression}

\begin{frame}

\frametitle{Logistic Regression: Likelihood Function}
Assuming two possible outcomes 1 and 0, the probability of being 1 is modeled as $$\mathnormal{p_i}=\frac{1}{1+\mathnormal{\exp(-\vec{\beta}\cdot\vec{x}^i)}}$$

The likelihood function is defined as 
\begin{align*}
\mathnormal{Likelihood}&=\mathnormal{\prod_{i=1}^{n}p_{i}^{y^i}(1-p_{i})^{1-y^{i}}}\\
\end{align*}

The log-likelihood function is the defined as the log transformation of the likelihood function
\begin{align*}
\mathnormal{\ell}=\mathnormal{\log(Likelihood)}
&=\mathnormal{\sum_{i=1}^{n}y^i\log(p_i)+(1-y^i)\log(1-p_i)}\\
\end{align*}

\end{frame}


\begin{frame}

\frametitle{Logistic Regression: Optimization Attempt}
It follows that
\begin{align*}
\mathnormal{\ell}&=\mathnormal{\sum_{i=1}^{n}y^i\log\frac{p_i}{1-p_i}+\log(1-p_i)}\\
&=\mathnormal{\sum_{i=1}^{n}y^i(\vec{\beta}\cdot\vec{x}^i)-\log(1+\exp(\vec{\beta}\cdot\vec{x}^i)))}
\end{align*}
Take the gradient against $\mathnormal{\beta}$s, we have
\begin{align*}
\mathnormal{\frac{\partial\ell}{\partial\beta_j}}
&=\mathnormal{\sum_{i=1}^{n}\left (y^i-\frac{1}{1+\exp(-\vec{\beta}\cdot\vec{x}^i)} \right) x_{j}^{i}}, \mathnormal{j=1, 2, 3, ...,m}
\end{align*}
$\beta$s can NOT be solved by setting $\nabla\ell=0$ because of the nonlinear term of $\mathnormal{x^i}$, which is $\mathnormal{\frac{1}{1+\exp(\vec{x}^i\cdot\vec{\beta})}}$.
\end{frame}

\begin{frame}

\subsection{Newton-Raphson Method}

\frametitle{Newton-Raphson Method for Optimizing Non-linear Functions}
Consider a function of one parameter $\mathnormal{\ell(\beta)}$ and assume $\mathnormal{\beta_0}$ is close to the point that minimizes $\mathnormal{\ell(\beta)}$. We can therefore use Talyor expansion for approximation

$$
\mathnormal{\ell(\beta)=\ell(\beta_0)+\ell'(\beta_0)(\beta-\beta_0)+\frac{1}{2}\ell''(\beta_0)(\beta-\beta_0)^2}
$$

The $\beta^*$ that minimize the function have derivative at the point 0 i.e. $\mathnormal{\ell'(\beta)|_{\beta=\beta^*}=0}$, by setting $\mathnormal{\ell'(\beta)}=0$, we get an iterative evaluation methods for $\beta^*$

$$
\mathnormal{\ell'(\beta_0)+\frac{1}{2}2\ell''(\beta_0)(\beta-\beta_0)}=0
\rightarrow
\mathnormal{\beta=\beta_0-\frac{\ell'(\beta_0)}{\ell''(\beta_0)}} \text{i.e.}
$$

$$
\mathnormal{\beta^{(k+1)}=\beta^{(k)}-\frac{\ell'(\beta^{(k)})}{\ell''(\beta^{(k)})}}
$$
\end{frame}

\begin{frame}
\frametitle{Multivariate Newton-Raphson Method }
For multivarite function, the iteration formula becomes
$$
\mathnormal{\beta^{(k+1)}}=\mathnormal{\beta^{(k)}}-\mathnormal{H}^{-1}\mathnormal{(\beta^{(k)})\nabla \ell(\beta^{(k)})}
$$
here $\mathnormal{H(\beta^{(k)})}$ is the Hessian matrix of $\ell(\beta)$ evaluated at $\mathnormal{\beta=\beta^{(k)}}$, defined as

$$
\mathnormal{H_{ab}}=\mathnormal{\frac{\partial^2\ell}{\partial\beta_a\partial\beta_b}|_{\beta=\beta^{(k)}}}
$$
and $\mathnormal{H}^{-1}\mathnormal{(\beta^{(k)})}$ is the inverse of $\mathnormal{H(\beta^{(k)})}$

\end{frame}

\begin{frame}

\frametitle{Logistic Regression}
Apply Newton-Raphson methods to optimize the logistic regression, we calculate the Hessian of the log-likelihood function
\begin{align*}
\mathnormal{\frac{\partial^2\ell}{\partial\beta_a\partial\beta_b}}
&=\mathnormal{-\sum_{i=1}^{n}x_b^i\frac{\exp(-\vec{\beta}\cdot\vec{x}^i)}{(1+\exp(-\vec{\beta}\cdot\vec{x}^i))^2}x_a^i}\\
&=\mathnormal{-\sum_{i=1}^{n}x_b^i p_i (1-p_i)x_a^i}\\
\end{align*}
written in matrix formula, the Hessian of the loglikelihood function is
\begin{align*}
\mathnormal{{H}}&=\mathnormal{-X^TWX}\text{, } 
\mathnormal{W}=  \begin{bmatrix}
    \mathnormal{p_1 (1-p_1)} & & \\
    & \ddots & \\
    & & \mathnormal{p_n (1-p_n)}
  \end{bmatrix}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Logistic Regression: Optimization Algorithm}
Use Newton Raphson Methods, we have
\begin{align*}
\mathnormal{\vec{\beta}^{(k+1)}} &\leftarrow \mathnormal{\vec{\beta}^{(k)}-{H}^{-1}\nabla\ell}\\
\mathnormal{\vec{\beta}^{(k+1)}} &\leftarrow \mathnormal{\vec{\beta}^{(k)}+({X}^T{W}{X})^{-1}{X}^T({y}-{p})}
\end{align*}

Recall in linear regression case
$$
\beta=(\mathnormal{X^T}\mathnormal{X})^{-1}\mathnormal{X^{T}}\mathnormal{y}
$$
\end{frame}
%
%
%\section{Generalized Linear Model}
%
%\begin{frame}
%\frametitle{Generalized Linear Model}
%Exponential family of probability density function
%$$
%\mathnormal{f(y)}=\exp\mathnormal{\left( \frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}
%$$
%The distribution have the following properties
%\begin{itemize}
%\item $\mathnormal{E(Y)}=\mathnormal{b'(\theta)}$
%\item $\mathnormal{Var(Y)}=\mathnormal{b''(\theta)a(\phi)}$
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%
%\frametitle{Generalized Linear Model: Gaussian}
%Gaussian distribution as a special case of exponential family 
%$$
%\mathnormal{f(y)}=\exp\mathnormal{\left( \frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-\frac{y^2}{2\sigma^2}-\frac{1}{2}\log(2\pi\sigma^2)\right)}
%$$
%
%where we have $\mathnormal{a(\phi)=\sigma^2}$, $\mathnormal{b(\mu)=\frac{1}{2}\mu^2}$
%Therefore
%\begin{itemize}
%\item $\mathnormal{E(Y)}=\mathnormal{b'(\theta)=\mu}$
%\item $\mathnormal{Var(Y)}=\mathnormal{b''(\theta)a(\phi)=\sigma^2}$
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Link Function}
%Assume a linear model where 
%\begin{align*}
%\mathnormal{\eta}&=\mathnormal{\vec{x}\cdot\vec{\beta}}\\
%\mathnormal{b'(\theta)}&=\mathnormal{g(\eta)}=\mathnormal{g(\vec{x}\cdot\vec{\beta})}\\ 
%\mathnormal{b''(\theta)}\frac{\partial \theta}{\partial \beta_j}&=\mathnormal{V(\mu)}\frac{\partial \theta}{\partial \beta_j}=\mathnormal{g'(x)x_j}
%\end{align*}
%here $\mathnormal{g^{-1}(\cdot)}$ is called the link function. $\mathnormal{b''(\theta)=V(\mu)}$
%
%\end{frame}
%
%\begin{frame}
%\frametitle{Log Likelihood Function of GLM}
%The log likelihood function of GLM
%
%$$\mathnormal{\ell}=\mathnormal{\sum_{i}\frac{y^i\theta^i-b(\theta^i)}{a(\phi)}+c^i(y^i, \phi)}$$
%
%In the model, only $\mathnormal{\theta}$ is depedent on $\mathnormal{\vec{x}\cdot\vec{\beta}}$. Therefore, "maximize" the likelihood function is equivalent to minimize  
%$$\mathnormal{\ell=-2\sum_{i} \left[y^i\theta^i-b(\theta^i)\right]}$$
%
%\end{frame}
%
%\begin{frame}
%\frametitle{Log Likelihood Function of GLM}
%
%Using Newton-Raphson method
%$$
%\mathnormal{\beta^{(m+1)}}=\mathnormal{\beta^{(m)}-H^{-1}(\beta^{(m)})\nabla \ell(\beta^{(m)})}
%$$
%We need to calculate the gradient of $\ell$ and its Hessian
%\end{frame}
%
%\begin{frame}
%\frametitle{The Gradient and Hessian}
%The gradient can be derived as
%$$\mathnormal{\frac{\partial \ell}{\partial \beta_j}}=\mathnormal{-2\sum_i(y^i-b'(\theta^i))\frac{\partial\theta^i}{\partial\beta_j}}$$
%
%$$\mathnormal{\frac{\partial \ell}{\partial \beta_j}}=\mathnormal{-2\sum_i(y^i-\mu^i)\frac{g'(\eta^i)}{V(\mu^i)}x_j^i}$$
%
%The hessian can be derived as
%$$\mathnormal{\frac{\partial^2 \ell}{\partial \beta_k\partial \beta_j}=2\sum_i\left[\frac{g'(\eta^i)^2}{V(\mu^i)}-(y^i-\mu^i)\frac{g''(\eta^i)V(\mu^i)-g'(\eta^i)^2V'(\mu^i)}{V(\mu^i)^2}\right]x_j^ix_k^i}$$
%\end{frame}
%
%
%\begin{frame}
%
%\frametitle{Optimization: Gradient Descent Method}
%Cost function $\mathnormal{J(\beta)}$
%
%Update methods
%$$\mathnormal{\beta_j\leftarrow \beta_j-\epsilon\frac{\partial}{\partial\beta_j}J(\beta)}$$
%where $\epsilon$ is the learning rate
%\end{frame}	
%
%
%\begin{frame}
%
%\frametitle{Gradient Descent Method for Linear Regression}
%Cost function $\mathnormal{J(\beta)}=\mathnormal{\sum_i}{\frac{1}{2}(\mathnormal{y}^i-\vec{\mathnormal{x}}^i\cdot\vec{\beta})^2}$
%
%$$\mathnormal{\frac{\partial J}{\partial \beta_j}}={\sum_i(\hat{\mathnormal{y}}^i-\mathnormal{y}^i)\mathnormal{x}_j^i}$$
%
%Update methods is now
%$$\beta_j\leftarrow \beta_j+\epsilon(\mathnormal{y}^i-\hat{\mathnormal{y}}^i)\mathnormal{x}_j^i$$
%
%The update method is quite intuitive considering that $\beta_i$ is adjusted higher if estimated $\hat{y}^j$ is less than $y^j$; adjusted lower if $\hat{y}^j$ is more than $y^j$
%
%\end{frame}	
%
%
%\begin{frame}
%
%\frametitle{Batch/Stochastic Gradient Descent}
%\textbf{Batch Gradient Descent}: if each step $\beta_i$ is updated using all data points
%
%$$\beta_j\leftarrow \beta_j+\sum_i\epsilon\frac{\partial}{\partial\beta_j}J(\beta)$$ or
%$$\beta_j\leftarrow \beta_j+\sum_i\epsilon(\mathnormal{y}^i-\hat{\mathnormal{y}}^i)\mathnormal{x}_j^i$$
%
%
%\textbf{Stochastic Gradient Descent}: if each step $\beta_i$ is updated using only one data point
%
%$$\beta_j\leftarrow \beta_j+\epsilon\frac{\partial}{\partial\beta_j}J(\beta)$$ or
%$$\beta_j\leftarrow \beta_j+\epsilon(y^i-\hat{y}^i)\mathnormal{x}_j^i$$
%
%
%\end{frame}	


\end{document}
