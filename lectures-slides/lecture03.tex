%!TeX encoding = UTF-8
%!TeX program = xelatex
\documentclass[notheorems, aspectratio=54]{beamer}
% aspectratio: 1610, 149, 54, 43(default), 32

\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{color,xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{lmodern} % 解决 font warning
% \usepackage[UTF8]{ctex}
\usepackage{animate} % insert gif

\usepackage{lipsum} % To generate test text 
\usepackage{ulem} % 下划线，波浪线

\usepackage{listings} % display code on slides; don't forget [fragile] option after \begin{frame}

% ----------------------------------------------
% tikx
\usepackage{framed}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{automata, calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate, 
     decoration={random steps, segment length=2.5cm, amplitude=.7mm}},
  torn border/.style={orange!30!black!5, decorate, 
     decoration={random steps, segment length=.5cm, amplitude=1.7mm}}}

% Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border] 
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}    
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]                % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \fill[torn border]               % Add the torn lower border
        ($(A.south east)-(0,.2)$) -- ($(A.south west)-(0,.2)$) -- 
        ($(A.south west)+(0,.2)$) -- ($(A.south east)+(0,.2)$) -- cycle;
  \fill[torn border]               % Add the torn upper border
        ($(A.north east)-(0,.2)$) -- ($(A.north west)-(0,.2)$) -- 
        ($(A.north west)+(0,.2)$) -- ($(A.north east)+(0,.2)$) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white, 
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

% ----------------------------------------------

\mode<presentation>{
    \usetheme{CambridgeUS}
    % Boadilla CambridgeUS
    % default Antibes Berlin Copenhagen
    % Madrid Montpelier Ilmenau Malmoe
    % Berkeley Singapore Warsaw
    \usecolortheme{beaver}
    % beetle, beaver, orchid, whale, dolphin
    \useoutertheme{infolines}
    % infolines miniframes shadow sidebar smoothbars smoothtree split tree
    \useinnertheme{circles}
    % circles, rectanges, rounded, inmargin
}
% 设置 block 颜色
\setbeamercolor{block title}{bg=red!30,fg=white}

\newcommand{\reditem}[1]{\setbeamercolor{item}{fg=red}\item #1}

% 缩放公式大小
\newcommand*{\Scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}

% 解决 font warning
\renewcommand\textbullet{\ensuremath{\bullet}}

% ---------------------------------------------------------------------
% flow chart
\tikzset{
    >=stealth',
    punktchain/.style={
        rectangle, 
        rounded corners, 
        % fill=black!10,
        draw=white, very thick,
        text width=6em,
        minimum height=2em, 
        text centered, 
        on chain
    },
    largepunktchain/.style={
        rectangle,
        rounded corners,
        draw=white, very thick,
        text width=10em,
        minimum height=2em,
        on chain
    },
    line/.style={draw, thick, <-},
    element/.style={
        tape,
        top color=white,
        bottom color=blue!50!black!60!,
        minimum width=6em,
        draw=blue!40!black!90, very thick,
        text width=6em, 
        minimum height=2em, 
        text centered, 
        on chain
    },
    every join/.style={->, thick,shorten >=1pt},
    decoration={brace},
    tuborg/.style={decorate},
    tubnode/.style={midway, right=2pt},
    font={\fontsize{10pt}{12}\selectfont},
}
% ---------------------------------------------------------------------

% code setting
\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{red},
    breaklines=true,
    xleftmargin=2em,
    numbers=left,
    numberstyle=\color[RGB]{222,155,81},
    frame=leftline,
    tabsize=4,
    breakatwhitespace=false,
    showspaces=false,               
    showstringspaces=false,
    showtabs=false,
    morekeywords={Str, Num, List},
}

% ---------------------------------------------------------------------

%% preamble
\title{Foundation of Analytics: Lecture 3}
% \subtitle{The subtitle}
\author{Dihui Lai}
\institute[WUSTL]{dlai@wustl.edu}

% -------------------------------------------------------------

\begin{document}

%% title frame
\begin{frame}
    \titlepage
\end{frame}


\begin{frame}
\frametitle{Content}

\begin{itemize}
\item Random Variables: Dependent, Independent, Correlation 
\item Linear Regression of One Variable
\item Linear Regression of Multiple Variables
\item Logistic Regression
\end{itemize}
\end{frame}


%% normal frame
\section{Random Variables: Dependent, Independent, Correlation }

\begin{frame}

\frametitle{Correlations between Random Variables}

Let's look at a few pairs of data  points?

\begin{itemize}
\item $\mathnormal{\vec{x}=[0.5, 0.6, 0.1, -0.3, 2.3], \vec{y}=[0.5, 0.6, 0.1, -0.3, 2.3]}$
\item $\mathnormal{\vec{x}=[0.5, 0.6, 0.1, -0.3, 2.3], \vec{y}=[0.6, 0.6, 0.12, -0.3, 2.3]}$
\item $\mathnormal{\vec{x}=[0.5, 0.6, 0.1, -0.3, 2.3], \vec{y}=[0.02, -0.2,0.2,2.1, -0.5]}$
\end{itemize}

What can you tell about the relationship between $\mathnormal{\vec{x}}$ and $\mathnormal{\vec{x}}$?

\end{frame}

\begin{frame}
\frametitle{Correlations between Random Variables}
Given two random variables $X$ and $Y$, denote the mean and variance of the two variables as $\mathnormal{E[X]=\mu_X}$, $\mathnormal{E[Y]=\mu_Y}$, $\mathnormal{Var[X]=\sigma^2_X}$, $\mathnormal{Var[Y]=\sigma^2_Y}$.

The covariance of X and Y is the number defined by
\begin{align*}
\mathnormal{Cov(X, Y)}&=\mathnormal{E[(X-\mu_X)(Y-\mu_Y)]}\\
&\mathnormal{=E[XY]-\mu_X \mu_Y}
\end{align*}

Empricial Estimation of Covariance
\begin{align*}
\mathnormal{Cov(X, Y)}&=\mathnormal{\frac{(x-\mu_x)(y^{T}-\mu_y)}{N}} (empirical)\\
\mathnormal{Cov(X, Y)}&=\frac{\mathnormal{(x-\mu_x)(y^{T}-\mu_y)}}{\mathnormal{N}-1} (unbiased)\\
\end{align*}


\end{frame}


\begin{frame}

\frametitle{Correlations between Random Variables}

The correlation of the two random variables is the number defined by 


\begin{align*}
\mathnormal{\rho_{XY}=\frac{Cov(X,Y)}{\sigma_X \sigma_Y}}
\end{align*}

\end{frame}
\begin{frame}
\frametitle{Correlations between Random Variables}
Calculate the covariance/correlation of 
\begin{itemize}
\item Example 1:$$\mathnormal{\vec{x}}=[2, -2, -2, 2], \mathnormal{\vec{y}}=[2, -2, -2, 2]$$
We have
$\mathnormal{\mu_x}=0$, $\mathnormal{\mu_y}=0$, $\mathnormal{\sigma_x^2=}4$, $\mathnormal{\sigma_y^2=}4$, $\mathnormal{E[XY]=}4$
Therefore $\mathnormal{Cov(X, Y)}=4-0=4$ and $\mathnormal{\rho_{xy}}=4/(2*2)=1$

\item Example 2: $$\mathnormal{\vec{x}}=[2, -2, -2, 2], \mathnormal{\vec{y}}=[2, 0, -2, 0]$$
We have
$\mathnormal{\mu_x}=0$, $\mathnormal{\mu_y}=0$, $\mathnormal{\sigma_x^2=}4$, $\mathnormal{\sigma_y^2=}2$, $\mathnormal{E[XY]=}2$
Therefore $\mathnormal{Cov(X, Y)}=2-0=2$ and $\mathnormal{\rho_{xy}}=2/(2*\sqrt{2})=1/\sqrt{2}$
\end{itemize}



\end{frame}

\section{Univariate LM}

\begin{frame}

\frametitle{Linear Regression with One Variable}
Data set:
$$
\mathnormal{y}=
\begin{bmatrix}
    \mathnormal{y^{1}}\\
    \mathnormal{y^{2}}\\
	\vdots\\
    \mathnormal{y^{n}}
\end{bmatrix}
,
\mathnormal{X}=
\begin{bmatrix}
    \mathnormal{x^{1}}\\
    \mathnormal{x^{2}}\\
	\vdots\\
    \mathnormal{x^{n}}
\end{bmatrix}
$$
\end{frame}

\begin{frame}
\frametitle{Linear Regression with One Variable}
Assume $\mathnormal{y}$ is linearly depending on $\mathnormal{x}$ i.e.

$$
\mathnormal{\hat{y} =\beta_0 + \beta_1 x}
$$

Find $\hat{\beta}$ that minimize the estimation error

$$
\mathnormal{\epsilon=\sum\limits_{i=1}^{n}(y^{i}-\hat{y}^{i})^2 = \sum\limits_{i=1}^{n} (y^{i}-{\beta_0}-{\beta_1} x^{i})^2}
$$

i.e.

$$
\mathnormal{\frac{\partial \epsilon}{\partial {\beta}_1}=0
\rightarrow
\sum\limits_{i=1}^{n}(y^{i}-{\beta_0}-{\beta_1} x^{i})x^{i}=0}
$$
$$
\mathnormal{
\frac{\partial \epsilon}{\partial {\beta}_0}=0
\rightarrow
\sum\limits_{i=1}^{n}(y^{i}-{\beta_0}-{\beta_1} x^{i})=0
}
$$

\end{frame}

\begin{frame}

$$
\mathnormal{
\beta_0\sum\limits_{i=1}^{n}x^{i}=\sum\limits_{i=1}^{n}y^{i}x^{i}-\beta_1\sum\limits_{i=1}^{n}x^{i}x^{i}
}
$$
\begin{align*}
\mathnormal{
{\beta_0}=\frac{1}{n}\sum\limits_{i=1}^{n}(y^{i}-{\beta_1} x^{i})=\bar{y}-{\beta_1} \bar{x}
}
\end{align*}

Insert the second equation to the first, we have

$$\mathnormal{n\bar{x}\bar{y}-{\beta_1}n\bar{x}\bar{x}=\sum\limits_{i=1}^{n}y^{i}x^{i}-{\beta_1}\sum\limits_{i=1}^{n}x^{i}x^{i}}
$$

Therefore, 
$$\mathnormal{
{\beta_1}=\frac{\frac{1}{n}\sum\limits_{i=1}^{n}x^{i}y^{i}-\bar{x}\bar{y}}{\frac{1}{n}\sum\limits_{i=1}^{n}x^{i}x^{i}-\bar{x}^2}=\frac{Cov(X,Y)}{Var(X)}=\rho_{XY}\frac{\sigma_{Y}}{\sigma_{X}}
}
$$
\end{frame}

\section{Multivariate LM}
\begin{frame}
\frametitle{Multivariate Linear Regression}
Data set:

$$
\begin{bmatrix}

\mathnormal{y, X}

\end{bmatrix}
=
\begin{bmatrix}
    \mathnormal{{y}^{1}}& \mathnormal{x^{1}_{0}}& \mathnormal{x^{1}_{1}} & \mathnormal{x^{1}_{2}} & \dots & \mathnormal{x^{1}_{m}} \\
    \mathnormal{{y}^{2}}& \mathnormal{x^{2}_{0}}& \mathnormal{x^{2}_{1}} & \mathnormal{x^{2}_{2}} & \dots & \mathnormal{x^{2}_{m}} \\
    	\vdots & \vdots     & \vdots & \vdots &\ddots &\vdots\\
    \mathnormal{{y}^{n}}& \mathnormal{x^{n}_{0}}& \mathnormal{x^{n}_{1}} & \mathnormal{x^{n}_{2}} & \dots & \mathnormal{x^{n}_{m}} \\
\end{bmatrix}
$$
\end{frame}

\begin{frame}
\frametitle{Multivariate Linear Regression}
Assume $\mathnormal{y}$ is a linear superposition of multiple $\mathnormal{x}$'s
    $$
	\mathnormal{\hat{y}=\beta_{0}+\beta_{1}x_1+\beta_{2}x_2+\hdots+\beta_{m}x_m}
    $$
    or simply
    $$
	\mathnormal{\hat{y}=\sum\limits_{j=1}^{m}\beta_j x_j}     
	$$
        

\end{frame}

\begin{frame}
\frametitle{Multivariate Linear Regression}
Estimate $\mathnormal{\beta}$'s that best fits the data, we need to minimize the error
    \begin{align*}
        \mathnormal{\epsilon}&=\mathnormal{\sum\limits_{i=1}^{n}(y^{i}-\hat{y}^{i})^2}\\
                &=\mathnormal{(y-\hat{y})^T(y-\hat{y})}
    \end{align*}
Use basic calculus we know, we want to have the  $\mathnormal{\beta}$s satisfy the following equation set:
$$
 \mathnormal{\frac{\partial \epsilon}{\partial \beta_j}=0, j=1, 2, 3, 4 ... m}
$$
\end{frame}

\begin{frame}
i.e.
$$
\mathnormal{\sum\limits_{i=1}^{n}\frac{\partial (y^{i}-\hat{y}^{i})^2}{\partial \beta_j}=0}
$$

$$
\mathnormal{\sum\limits_{i=1}^{n}(y^{i}-\hat{y}^{i})\frac{\partial\hat{y}^{i}}{\partial \beta_j}=0}
$$

$$
\mathnormal{\sum\limits_{i=1}^{n}(y^{i}-\hat{y}^{i})x^{i}_{j}=0}
$$

\end{frame}
\begin{frame}

\frametitle{Multivariate Linear Regression}

Written in matrix formula we require

$$
\mathnormal{(y-X \beta)}^{T}\mathnormal{X}=0
$$
or after transposing
$$
\mathnormal{X^{T}}\mathnormal{y}-\mathnormal{X^T}\mathnormal{X}\beta=0
$$

Therefore 

$$
\beta=(\mathnormal{X^T}\mathnormal{X})^{-1}\mathnormal{X^{T}}\mathnormal{y}
$$

\end{frame}

\section{Logistic Regression}

\begin{frame}

\frametitle{Logistic Regression: Likelihood Function}
Assuming two possible outcomes 1 and 0, the probability of being 1 is modeled as $$\mathnormal{p_i}=\frac{1}{1+\mathnormal{\exp(-\vec{\beta}\cdot\vec{x}^i)}}$$

The likelihood function is defined as 
\begin{align*}
\mathnormal{Likelihood}&=\mathnormal{\prod_{i=1}^{n}p_{i}^{y^i}(1-p_{i})^{1-y^{i}}}\\
\end{align*}

The log-likelihood function is the defined as the log transformation of the likelihood function
\begin{align*}
\mathnormal{\ell}=\mathnormal{\log(Likelihood)}
&=\mathnormal{\sum_{i=1}^{n}y^i\log(p_i)+(1-y^i)\log(1-p_i)}\\
\end{align*}

\end{frame}


\begin{frame}

\frametitle{Logistic Regression: Optimization Attempt}
It follows that
\begin{align*}
\mathnormal{\ell}&=\mathnormal{\sum_{i=1}^{n}y^i\log\frac{p_i}{1-p_i}+\log(1-p_i)}\\
&=\mathnormal{\sum_{i=1}^{n}y^i(\vec{\beta}\cdot\vec{x}^i)-\log(1+\exp(\vec{\beta}\cdot\vec{x}^i)))}
\end{align*}
Take the gradient against $\mathnormal{\beta}$s, we have
\begin{align*}
\mathnormal{\frac{\partial\ell}{\partial\beta_j}}
&=\mathnormal{\sum_{i=1}^{n}\left (y^i-\frac{1}{1+\exp(-\vec{\beta}\cdot\vec{x}^i)} \right) x_{j}^{i}}, \mathnormal{j=1, 2, 3, ...,m}
\end{align*}
$\beta$s can NOT be solved by setting $\nabla\ell=0$ because of the nonlinear term of $\mathnormal{x^i}$, which is $\mathnormal{\frac{1}{1+\exp(\vec{x}^i\cdot\vec{\beta})}}$.
\end{frame}

\begin{frame}

\subsection{Newton-Raphson Method}

\frametitle{Newton-Raphson Method for Optimizing Non-linear Functions}
Consider a function of one parameter $\mathnormal{\ell(\beta)}$ and assume $\mathnormal{\beta_0}$ is close to the point that minimizes $\mathnormal{\ell(\beta)}$. We can therefore use Talyor expansion for approximation

$$
\mathnormal{\ell(\beta)=\ell(\beta_0)+\ell'(\beta_0)(\beta-\beta_0)+\frac{1}{2}\ell''(\beta_0)(\beta-\beta_0)^2}
$$

The $\beta^*$ that minimize the function have derivative at the point 0 i.e. $\mathnormal{\ell'(\beta)|_{\beta=\beta^*}=0}$, by setting $\mathnormal{\ell'(\beta)}=0$, we get an iterative evaluation methods for $\beta^*$

$$
\mathnormal{\ell'(\beta_0)+\frac{1}{2}2\ell''(\beta_0)(\beta-\beta_0)}=0
\rightarrow
\mathnormal{\beta=\beta_0-\frac{\ell'(\beta_0)}{\ell''(\beta_0)}} \text{i.e.}
$$

$$
\mathnormal{\beta^{(k+1)}=\beta^{(k)}-\frac{\ell'(\beta^{(k)})}{\ell''(\beta^{(k)})}}
$$
\end{frame}

\begin{frame}
\frametitle{Multivariate Newton-Raphson Method }
For multivarite function, the iteration formula becomes
$$
\mathnormal{\beta^{(k+1)}}=\mathnormal{\beta^{(k)}}-\mathnormal{H}^{-1}\mathnormal{(\beta^{(k)})\nabla \ell(\beta^{(k)})}
$$
here $\mathnormal{H(\beta^{(k)})}$ is the Hessian matrix of $\ell(\beta)$ evaluated at $\mathnormal{\beta=\beta^{(k)}}$, defined as

$$
\mathnormal{H_{ab}}=\mathnormal{\frac{\partial^2\ell}{\partial\beta_a\partial\beta_b}|_{\beta=\beta^{(k)}}}
$$
and $\mathnormal{H}^{-1}\mathnormal{(\beta^{(k)})}$ is the inverse of $\mathnormal{H(\beta^{(k)})}$

\end{frame}

\begin{frame}

\frametitle{Logistic Regression}
Apply Newton-Raphson methods to optimize the logistic regression, we calculate the Hessian of the log-likelihood function
\begin{align*}
\mathnormal{\frac{\partial^2\ell}{\partial\beta_a\partial\beta_b}}
&=\mathnormal{-\sum_{i=1}^{n}x_b^i\frac{\exp(-\vec{\beta}\cdot\vec{x}^i)}{(1+\exp(-\vec{\beta}\cdot\vec{x}^i))^2}x_a^i}\\
&=\mathnormal{-\sum_{i=1}^{n}x_b^i p_i (1-p_i)x_a^i}\\
\end{align*}
written in matrix formula, the Hessian of the loglikelihood function is
\begin{align*}
\mathnormal{{H}}&=\mathnormal{-X^TWX}\text{, } 
\mathnormal{W}=  \begin{bmatrix}
    \mathnormal{p_1 (1-p_1)} & & \\
    & \ddots & \\
    & & \mathnormal{p_n (1-p_n)}
  \end{bmatrix}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Logistic Regression: Optimization Algorithm}
Use Newton Raphson Methods, we have
\begin{align*}
\mathnormal{\vec{\beta}^{(k+1)}} &\leftarrow \mathnormal{\vec{\beta}^{(k)}-{H}^{-1}\nabla\ell}\\
\mathnormal{\vec{\beta}^{(k+1)}} &\leftarrow \mathnormal{\vec{\beta}^{(k)}+({X}^T{W}{X})^{-1}{X}^T({y}-{p})}
\end{align*}

Recall in linear regression case
$$
\beta=(\mathnormal{X^T}\mathnormal{X})^{-1}\mathnormal{X^{T}}\mathnormal{y}
$$
\end{frame}


\end{document}
